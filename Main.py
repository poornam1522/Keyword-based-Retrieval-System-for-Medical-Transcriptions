# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1twWsQdKRdnRS4JPNiub0OJgs7EPO8u78
"""

import pandas as pd
import numpy as np

filepath = '/content/mtsamples.csv'
data = pd.read_csv(filepath)
data.head()

"""# Data Preprocessing"""

# Checking basic information about the dataset
data.info()

# Summary statistics for numerical columns (if any)
data.describe()

# Get an overview of categorical columns
print(data['medical_specialty'].value_counts())

# Checking the shape of the dataset (rows, columns)
print(data.shape)

# Checking for missing values
print(data.isnull().sum())

# Drop rows where transcription is missing
data = data.dropna(subset=['transcription'])

# Verify
print(f"Rows remaining after dropping missing transcriptions: {data.shape[0]}")

# Fill missing keywords with 'Unknown'
data['keywords'] = data['keywords'].fillna('Unknown')

# Verify there are no more missing values in 'keywords'
print(data['keywords'].isnull().sum())

# Analyze the most common medical specialties
import matplotlib.pyplot as plt

data['medical_specialty'].value_counts().plot(kind='bar', figsize=(12,6), title='Most Common Medical Specialties')
plt.show()

# Tokenize the keywords column to analyze frequency
from collections import Counter

# Tokenize the keywords (split by commas or spaces)
keywords_list = data['keywords'].apply(lambda x: x.split(', '))
keywords_flat = [item for sublist in keywords_list for item in sublist]

# Count the frequency of each keyword
keyword_counts = Counter(keywords_flat)
print(f"Top 10 most common keywords: {keyword_counts.most_common(10)}")

# Plot the most common keywords
pd.Series(dict(keyword_counts.most_common(10))).plot(kind='bar', title='Top 10 Keywords', figsize=(12,6))
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer

# Select rows where keywords are 'Unknown'
unknown_keywords = data[data['keywords'] == 'Unknown']

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=5, stop_words='english')  # Adjust max_features based on how many keywords you want

# Fit the TF-IDF model on the transcription column where keywords are unknown
tfidf_matrix = tfidf.fit_transform(unknown_keywords['transcription'])

# Get the feature names (keywords) from TF-IDF
tfidf_feature_names = tfidf.get_feature_names_out()

# Replace 'Unknown' keywords with the top 5 most important words from the transcription
def get_top_keywords(row_index):
    row = tfidf_matrix[row_index]
    top_n_keywords = [tfidf_feature_names[i] for i in row.nonzero()[1]]
    return ', '.join(top_n_keywords)

# Apply the function to each row where keywords are 'Unknown'
# Use unknown_keywords.index to iterate through the correct index
for index in unknown_keywords.index:
    data.at[index, 'keywords'] = get_top_keywords(unknown_keywords.index.get_loc(index))

# Verify if the 'Unknown' keywords have been replaced
print(data['keywords'].value_counts())

# Handle blank entries in the 'keywords' column
data['keywords'] = data['keywords'].replace('', 'no_keywords')

# Check if there are any remaining blank entries
print(f"Remaining blank keywords: {data[data['keywords'] == ''].shape[0]}")

print(data['keywords'].isnull().sum())

print(data['keywords'].values)

# Check the top 10 most frequent keywords again
data['keywords'].value_counts().head(10)

# Check the most common medical specialties
data['medical_specialty'].value_counts().head(10)

pip install rake-nltk

import nltk

# Download the punkt tokenizer
nltk.download('punkt')

from rake_nltk import Rake

# Step 1: Initialize RAKE
rake = Rake()

# Step 2: Define a function to extract keywords using RAKE
def extract_rake_keywords(text):
    rake.extract_keywords_from_text(text)
    return ', '.join(rake.get_ranked_phrases())

# Step 3: Apply the function on the combined text of 'description' and 'transcription'
data['combined_text'] = data['transcription'].fillna('') + ' ' + data['description'].fillna('')
data['rake_keywords'] = data['combined_text'].apply(extract_rake_keywords)

# Step 4: Review the extracted keywords
data[['description', 'transcription', 'rake_keywords']].head()

# Ensure there are no null values in 'rake_keywords'
data['rake_keywords'] = data['rake_keywords'].fillna('')

# Define a function to tokenize the keywords
def tokenize(text):
    return text.split(', ')

# Prepare a list of tokenized documents
documents = data['rake_keywords'].apply(tokenize).tolist()

# Check the first few documents
print(documents[:5])

"""# Implementing Search Functionality"""

pip install rank_bm25

from rank_bm25 import BM25Okapi
import numpy as np

# Initialize BM25 with the tokenized documents
bm25 = BM25Okapi(documents)

# Define a function to perform search
def search(query, bm25_model):
    query_tokens = tokenize(query)
    scores = bm25_model.get_scores(query_tokens)
    return scores

# Example search
query = "pain, patient"
scores = search(query, bm25)
ranked_indices = np.argsort(scores)[::-1]
top_documents = data.iloc[ranked_indices].head(10)  # Get top 10 documents
print(top_documents[['sample_name', 'rake_keywords']])

pip install streamlit

import streamlit as st
from rank_bm25 import BM25Okapi
import pandas as pd
import numpy as np

# Load your dataset
filepath = '/content/mtsamples.csv'
data = pd.read_csv(filepath)

# Tokenize function
def tokenize(text):
    return text.split(', ')

# Prepare the documents
# Check if 'rake_keywords' exists, if not try 'keywords'
if 'rake_keywords' in data.columns:
    data['rake_keywords'] = data['rake_keywords'].fillna('')
    documents = data['rake_keywords'].apply(tokenize).tolist()
elif 'keywords' in data.columns:
    data['keywords'] = data['keywords'].fillna('')
    documents = data['keywords'].apply(tokenize).tolist()
else:
    # Handle the case where neither column exists
    st.write("Error: Could not find 'rake_keywords' or 'keywords' column in the dataset.")
    st.stop()

# Initialize BM25
bm25 = BM25Okapi(documents)

# Define the search function
def search(query, bm25_model):
    query_tokens = tokenize(query)
    scores = bm25_model.get_scores(query_tokens)
    return scores

# Streamlit app
st.title('Medical Transcription Search Engine')

st.write("Enter keywords to search for relevant medical transcriptions.")

# User input
query = st.text_input("Search Query")

if query:
    # Perform search
    scores = search(query, bm25)
    ranked_indices = np.argsort(scores)[::-1]
    top_documents = data.iloc[ranked_indices].head(10)  # Get top 10 documents

    # Display results
    st.write("Top results:")
    for idx, row in top_documents.iterrows():
        # Check for the correct column name to display
        if 'rake_keywords' in data.columns:
            st.write(f"**Sample Name:** {row['sample_name']}")
            st.write(f"**Keywords:** {row['rake_keywords']}")
        elif 'keywords' in data.columns:
            st.write(f"**Sample Name:** {row['sample_name']}")
            st.write(f"**Keywords:** {row['keywords']}")
        st.write("---")

streamlit run app.py

